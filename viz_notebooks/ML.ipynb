{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODOOs\n",
    "- clean up code\n",
    "- learning rate discussion\n",
    "- make a consisten train and test df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A notebook for parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import gensim\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU:\n",
      "Quadro T1000 with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using GPU:\")\n",
    "    print(torch.cuda.get_device_name(0))  # 0 is the index of the GPU\n",
    "else:\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Inputs (not sure why shape mismatch TODO check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_news Real\n",
      "0    0.522985\n",
      "1    0.477015\n",
      "Name: proportion, dtype: float64\n",
      "df_training: Real\n",
      "0    0.524278\n",
      "1    0.475722\n",
      "Name: proportion, dtype: float64\n",
      "df_test Real\n",
      "0    0.517817\n",
      "1    0.482183\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_news = pd.read_pickle('..\\input\\df_news.pkl')\n",
    "\n",
    "# Use train_test_split for a more consistent and reproducible split\n",
    "df_training, df_test = train_test_split(df_news, test_size=0.2, random_state=42)\n",
    "\n",
    "# check that the df is balanced\n",
    "print('df_news:', df_news['Real'].value_counts(normalize=True))\n",
    "print('df_training:', df_training['Real'].value_counts(normalize=True))\n",
    "print('df_test:', df_test['Real'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader # this is for data loading\n",
    "\n",
    "df_training = df_training[['vector', 'subject', 'Real']]\n",
    "df_test = df_test[['vector', 'subject', 'Real']]\n",
    "\n",
    "# apply encoding consistently\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# fit LabelEncoder on the training set and transform both sets\n",
    "df_training['subject'] = label_encoder.fit_transform(df_training['subject'])\n",
    "df_test['subject'] = label_encoder.transform(df_test['subject'])  # Use transform, not fit_transform\n",
    "\n",
    "# we make a set of features for vector+subject and then a set of feature for vector only\n",
    "class VectorizedTextDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        feature = torch.tensor(self.features[idx], dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return feature, label\n",
    "    \n",
    "def create_data(df, mode='subj', batch_size=2, shuffle=False):\n",
    "    # Get the vectors (Word2Vec or equivalent)\n",
    "    X_vectors = np.array(df['vector'].tolist())\n",
    "\n",
    "    # If mode is 'subj', include subject features\n",
    "    if mode == 'subj':\n",
    "        df_features = df.drop(columns=['vector', 'Real'])\n",
    "        X_features = df_features.values\n",
    "        X = np.hstack([X_vectors, X_features])  # Concatenate vectors with subject features\n",
    "    else:\n",
    "        X = X_vectors  # Only use vectors\n",
    "    \n",
    "    y = np.array(df['Real'].tolist())  # Labels (1 or 0)\n",
    "    \n",
    "    # Create Dataset and DataLoader\n",
    "    input_dataset = VectorizedTextDataset(X, y)\n",
    "    input_loader = DataLoader(input_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    \n",
    "    return input_loader, X, y\n",
    "\n",
    "# Create DataLoaders for training and test sets\n",
    "train_loader_subj, X_train_subj, y_train_subj = create_data(df_training, mode='subj')\n",
    "test_loader_subj, X_test_subj, y_test_subj = create_data(df_test, mode='subj')\n",
    "\n",
    "train_loader, X_train, Y_train = create_data(df_training, mode='')\n",
    "test_loader, x_test, y_test = create_data(df_test, mode='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple feed-forward neural network for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module): # create a class with the neural networ module\n",
    "    def __init__(self, input_dim): # input dimensions\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, 1) # nn.Linear (input, output) - dimensions of input (features) and output (labels)\n",
    "        self.sigmoid = nn.Sigmoid() # maps the output onto a 0 to 1 line, representing the probability of it belonging to a 1 \n",
    "    \n",
    "    def forward(self, x):  # forward nn \n",
    "        x = self.fc(x) # first the input is passed through the fully connected linear layer \n",
    "        x = self.sigmoid(x) # then it goes through the sigmoid \n",
    "        return x\n",
    "    \n",
    "def model_training(input_loader, X_train, learning_rate, epochs):\n",
    "    # Model, Loss, Optimizer\n",
    "    input_dim = X_train.shape[1]  # Number of features - 100 since this is the # components of the vector\n",
    "    model = TextClassifier(input_dim)\n",
    "    criterion = nn.BCELoss()  # loss function minimise \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # optimizer - lr learning rate, a critical hyperparameter that controls how large or small the updates to the weights will be at each step\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):  # choose epochs - one complete pass through the entire dataset\n",
    "        model.train()\n",
    "        for features, labels in input_loader:  # input_loader is train_loader\n",
    "            optimizer.zero_grad() #  reset the gradients for each new epoch \n",
    "            outputs = model(features) \n",
    "            loss = criterion(outputs.squeeze(), labels.float())  # compute the loss\n",
    "            loss.backward()\n",
    "            optimizer.step() # update the weights \n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, loader, threshold):\n",
    "    model.eval() # model is in evaluate mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for features, labels in loader: # the loader is the dataset\n",
    "            outputs = model(features) # get the outputs\n",
    "            predicted = (outputs.squeeze() > threshold).long()  # predicted label - it is assigned to whichever has a probability >0.5 (threshold can be changed)\n",
    "            correct += (predicted == labels).sum().item() \n",
    "            total += labels.size(0)\n",
    "    accuracy = correct / total\n",
    "    print(f'Accuracy {accuracy}')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.00018206570530310273\n",
      "Epoch 2, Loss: 4.6553395804949105e-05\n",
      "Epoch 3, Loss: 5.472005796036683e-05\n",
      "Epoch 4, Loss: 6.729816959705204e-05\n",
      "Epoch 5, Loss: 6.0085090808570385e-05\n",
      "Epoch 1, Loss: 3.713506885105744e-05\n",
      "Epoch 2, Loss: 5.006815172237111e-06\n",
      "Epoch 3, Loss: 4.768373855768004e-07\n",
      "Epoch 4, Loss: 4.768373855768004e-07\n",
      "Epoch 5, Loss: 2.0861668872385053e-06\n"
     ]
    }
   ],
   "source": [
    "# train the model with the subject\n",
    "model = model_training(train_loader, X_train, learning_rate=0.1, epochs=5)\n",
    "model_subj = model_training(train_loader_subj, X_train_subj, learning_rate=0.1, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.9333760231638732\n",
      "Accuracy 0.9400579096831672\n"
     ]
    }
   ],
   "source": [
    "# evaluate model performances \n",
    "model_eval_subj = evaluate_model(model_subj, train_loader_subj, threshold=0.5)\n",
    "model_eval = evaluate_model(model, train_loader, threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_lr(learningrate):\n",
    "    # train the model with the subject\n",
    "    model = model_training(train_loader, X_train, learning_rate=learningrate, epochs=5)\n",
    "    model_subj = model_training(train_loader_subj, X_train_subj, learning_rate=learningrate, epochs=5)\n",
    "    model_eval_subj = evaluate_model(model_subj, train_loader_subj, threshold=0.5)\n",
    "    model_eval = evaluate_model(model, train_loader, threshold=0.5)\n",
    "    return\n",
    "\n",
    "new_lr(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing how the code converges, changing the number of epochs would not make it improve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score \n",
    "def load_xgboost_data(df):\n",
    "    \n",
    "    return\n",
    "model_xgboost = 1\n",
    "y_pred = model_xgboost.predict(X)\n",
    "# xgboost comparison\n",
    "accuracy = accuracy_score(y_actual, y_pred)\n",
    "f1 = f1_score(y_actual, y_pred)\n",
    "precision = precision_score(y_actual, y_pred)\n",
    "recall = recall_score(y_actual, y_pred)\n",
    "cm = confusion_matrix(y_actual, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Testing improved accuracy with the extra \"subject\" category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly using the category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jlari\\AppData\\Local\\Temp\\ipykernel_32320\\3103970089.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_training['subject'] = label_encoder.fit_transform(df_training['subject'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.06220392882823944\n",
      "Epoch 2, Loss: 0.4582524299621582\n",
      "Epoch 3, Loss: 0.001292006578296423\n",
      "Epoch 4, Loss: 0.0012848366750404239\n",
      "Epoch 5, Loss: 0.002674767514690757\n",
      "Train Accuracy: 0.9583774152235648\n",
      "Test Accuracy: 0.9587973273942093\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader # this is for data loading\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class VectorizedTextDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        feature = torch.tensor(self.features[idx], dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return feature, label\n",
    "    \n",
    "df_training = df_news[['vector', 'subject', 'Real']]\n",
    "label_encoder = LabelEncoder()\n",
    "df_training['subject'] = label_encoder.fit_transform(df_training['subject'])\n",
    "\n",
    "# Convert features to numpy arrays\n",
    "# X = np.array(df_training['vector'].tolist()) # features - array of the vectors \n",
    "X_vectors = np.array(df_training['vector'].tolist())\n",
    "df_features = df_training.drop(columns=['vector', 'Real'])\n",
    "\n",
    "# Convert features to numpy array\n",
    "X_features = df_features.values\n",
    "\n",
    "# Combine word2vec vectors with features\n",
    "X = np.hstack([X_vectors, X_features])\n",
    "y = np.array(df_training['Real'].tolist()) # label - array of the 1 or 0s for each vector\n",
    "\n",
    "# Split data into test and train - here we do an 80/20 split. Optional: additional validation set \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create Dataset instances\n",
    "train_dataset = VectorizedTextDataset(X_train, y_train) # get the data in a structured dataset (abstract class that wraps around). \n",
    "test_dataset = VectorizedTextDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoader instances\n",
    "# this is for loading the data onto the GPU\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "class TextClassifier(nn.Module): # create a class with the neural networ module\n",
    "    def __init__(self, input_dim): # input dimensions\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, 1) # nn.Linear (input, output) - dimensions of input (features) and output (labels)\n",
    "        self.sigmoid = nn.Sigmoid() # maps the output onto a 0 to 1 line, representing the probability of it belonging to a 1 \n",
    "    \n",
    "    def forward(self, x):  # forward nn \n",
    "        x = self.fc(x) # first the input is passed through the fully connected linear layer \n",
    "        x = self.sigmoid(x) # then it goes through the sigmoid \n",
    "        return x\n",
    "    \n",
    "# Model, Loss, Optimizer\n",
    "input_dim = X_train.shape[1]  # Number of features - 100 since this is the # components of the vector\n",
    "model = TextClassifier(input_dim)\n",
    "criterion = nn.BCELoss()  # loss function minimise \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # optimizer - lr learning rate, a critical hyperparameter that controls how large or small the updates to the weights will be at each step\n",
    "# Adam optimizer\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):  # choose epochs - one complete pass through the entire dataset\n",
    "    model.train()\n",
    "    for features, labels in train_loader:\n",
    "        optimizer.zero_grad() #  reset the gradients for each new epoch \n",
    "        outputs = model(features) \n",
    "        loss = criterion(outputs.squeeze(), labels.float())  # compute the loss\n",
    "        loss.backward()\n",
    "        optimizer.step() # update the weights \n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "def evaluate_model(model, loader):\n",
    "    model.eval() # model is in evaluate mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for features, labels in loader: # the loader is the dataset\n",
    "            outputs = model(features) # get the outputs\n",
    "            predicted = (outputs.squeeze() > 0.5).long()  # predicted label - it is assigned to whichever has a probability >0.5 (threshold can be changed)\n",
    "            correct += (predicted == labels).sum().item() \n",
    "            total += labels.size(0)\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "print(f'Train Accuracy: {evaluate_model(model, train_loader)}')\n",
    "print(f'Test Accuracy: {evaluate_model(model, test_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now not using the category (this is the final model saved as forward_nn)\n",
    "\n",
    "FOR A FAIR COMPARISON YOU WOULD NEED TO USE THE SAME TRAIN AND TEST SET EACH TIME, OR RETRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jlari\\AppData\\Local\\Temp\\ipykernel_32320\\681567642.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_nosbj.load_state_dict(torch.load(r'..\\models\\forward_nn.pth'))\n"
     ]
    }
   ],
   "source": [
    "# reload model \n",
    "\n",
    "# Define the model class\n",
    "class TextClassifier_nosbj(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(TextClassifier_nosbj, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "input_dim = 100  # Set this to your input dimension\n",
    "num_classes = 2  # Binary classification\n",
    "model_nosbj = TextClassifier_nosbj(input_dim, num_classes)\n",
    "# encode the \"subject\" feature\n",
    "# Convert features to numpy arrays\n",
    "X_nosb = np.array(df_training['vector'].tolist())  # Features - array of vectors\n",
    "y_nosb = np.array(df_training['Real'].tolist())    # Labels - array of 1s or 0s\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train_nosb, X_test_nosb, y_train_nosb, y_test_nosb = train_test_split(X_nosb, y_nosb, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create Dataset instances\n",
    "test_dataset_nosb = VectorizedTextDataset(X_test_nosb, y_test_nosb)\n",
    "\n",
    "# Create DataLoader instances\n",
    "test_loader_nosb = DataLoader(test_dataset_nosb, batch_size=2, shuffle=False)\n",
    "# Load the model\n",
    "model_nosbj.load_state_dict(torch.load(r'..\\models\\forward_nn.pth'))\n",
    "model_nosbj.eval()\n",
    "\n",
    "model_nosbj.eval() # model is in evaluate mode\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for features, labels in test_loader_nosb: # the loader is the dataset\n",
    "        outputs = model_nosbj(features) # get the outputs\n",
    "        predicted = (outputs.squeeze() > 0.5).long()  # predicted label - it is assigned to whichever has a probability >0.5 (threshold can be changed)\n",
    "        correct += (predicted == labels).sum().item() \n",
    "        total += labels.size(0)\n",
    "accuracy = correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jlari\\AppData\\Local\\Temp\\ipykernel_8648\\3048843464.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_training['subject'] = label_encoder.fit_transform(df_training['subject'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.06339212507009506\n",
      "Epoch 2, Loss: 0.4413672387599945\n",
      "Epoch 3, Loss: 0.14739127457141876\n",
      "Epoch 4, Loss: 1.3124511241912842\n",
      "Epoch 5, Loss: 0.020572401583194733\n",
      "Epoch 6, Loss: 0.0014195336261764169\n",
      "Epoch 7, Loss: 0.04348200559616089\n",
      "Epoch 8, Loss: 0.32501399517059326\n",
      "Epoch 9, Loss: 0.01063751894980669\n",
      "Epoch 10, Loss: 0.013403023593127728\n",
      "Train Accuracy: 0.9454034188986024\n",
      "Test Accuracy: 0.9479955456570156\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader # this is for data loading\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class VectorizedTextDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        feature = torch.tensor(self.features[idx], dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return feature, label\n",
    "    \n",
    "df_training = df_news[['vector', 'subject', 'Real']]\n",
    "label_encoder = LabelEncoder()\n",
    "df_training['subject'] = label_encoder.fit_transform(df_training['subject'])\n",
    "\n",
    "# Convert features to numpy arrays\n",
    "# X = np.array(df_training['vector'].tolist()) # features - array of the vectors \n",
    "X_vectors = np.array(df_training['vector'].tolist())\n",
    "df_features = df_training.drop(columns=['vector', 'Real'])\n",
    "\n",
    "# Convert features to numpy array\n",
    "X_features = df_features.values\n",
    "\n",
    "# Combine word2vec vectors with features\n",
    "X = np.hstack([X_vectors, X_features])\n",
    "y = np.array(df_training['Real'].tolist()) # label - array of the 1 or 0s for each vector\n",
    "\n",
    "# Split data into test and train - here we do an 80/20 split. Optional: additional validation set \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create Dataset instances\n",
    "train_dataset = VectorizedTextDataset(X_train, y_train) # get the data in a structured dataset (abstract class that wraps around). \n",
    "test_dataset = VectorizedTextDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoader instances\n",
    "# this is for loading the data onto the GPU\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "class TextClassifier(nn.Module): # create a class with the neural networ module\n",
    "    def __init__(self, input_dim): # input dimensions\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, 1) # nn.Linear (input, output) - dimensions of input (features) and output (labels)\n",
    "        self.sigmoid = nn.Sigmoid() # maps the output onto a 0 to 1 line, representing the probability of it belonging to a 1 \n",
    "    \n",
    "    def forward(self, x):  # forward nn \n",
    "        x = self.fc(x) # first the input is passed through the fully connected linear layer \n",
    "        x = self.sigmoid(x) # then it goes through the sigmoid \n",
    "        return x\n",
    "    \n",
    "# Model, Loss, Optimizer\n",
    "input_dim = X_train.shape[1]  # Number of features - 100 since this is the # components of the vector\n",
    "model = TextClassifier(input_dim)\n",
    "criterion = nn.BCELoss()  # loss function minimise \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # optimizer - lr learning rate, a critical hyperparameter that controls how large or small the updates to the weights will be at each step\n",
    "# Adam optimizer\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):  # choose epochs - one complete pass through the entire dataset\n",
    "    model.train()\n",
    "    for features, labels in train_loader:\n",
    "        optimizer.zero_grad() #  reset the gradients for each new epoch \n",
    "        outputs = model(features) \n",
    "        loss = criterion(outputs.squeeze(), labels.float())  # compute the loss\n",
    "        loss.backward()\n",
    "        optimizer.step() # update the weights \n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "def evaluate_model(model, loader):\n",
    "    model.eval() # model is in evaluate mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for features, labels in loader: # the loader is the dataset\n",
    "            outputs = model(features) # get the outputs\n",
    "            predicted = (outputs.squeeze() > 0.5).long()  # predicted label - it is assigned to whichever has a probability >0.5 (threshold can be changed)\n",
    "            correct += (predicted == labels).sum().item() \n",
    "            total += labels.size(0)\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "print(f'Train Accuracy: {evaluate_model(model, train_loader)}')\n",
    "print(f'Test Accuracy: {evaluate_model(model, test_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would adding a hidden layer improve the accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.10727664083242416\n",
      "Epoch 2, Loss: 0.00014839951472822577\n",
      "Epoch 3, Loss: 0.009782358072698116\n",
      "Epoch 4, Loss: 0.00034248080919496715\n",
      "Epoch 5, Loss: 0.00020904683333355933\n",
      "Train Accuracy: 0.9564563728492678\n",
      "Test Accuracy: 0.9555679287305122\n"
     ]
    }
   ],
   "source": [
    "class TextClassifier_hiddenlayer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(TextClassifier_hiddenlayer, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)  # First layer (input to hidden)\n",
    "        self.relu = nn.ReLU()  # Activation function for the hidden layer\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)  # Second layer (hidden to output)\n",
    "        self.sigmoid = nn.Sigmoid()  # Sigmoid for binary classification\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)  # Pass input through the first linear layer (input to hidden)\n",
    "        x = self.relu(x)  # Apply ReLU activation\n",
    "        x = self.fc2(x)  # Pass through the second linear layer (hidden to output)\n",
    "        x = self.sigmoid(x)  # Apply sigmoid activation for output\n",
    "        return x\n",
    "    \n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 100  # Example value, you can adjust it\n",
    "model_2 = TextClassifier_hiddenlayer(input_dim, hidden_dim)\n",
    "\n",
    "criterion = nn.BCELoss()  # loss function minimise \n",
    "optimizer = optim.Adam(model_2.parameters(), lr=0.001)  # optimizer - lr learning rate, a critical hyperparameter that controls how large or small the updates to the weights will be at each step\n",
    "# Adam optimizer\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):  # choose epochs - one complete pass through the entire dataset\n",
    "    model_2.train()\n",
    "    for features, labels in train_loader:\n",
    "        optimizer.zero_grad() #  reset the gradients for each new epoch \n",
    "        outputs = model_2(features) \n",
    "        loss = criterion(outputs.squeeze(), labels.float())  # compute the loss\n",
    "        loss.backward()\n",
    "        optimizer.step() # update the weights \n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "print(f'Train Accuracy: {evaluate_model(train_loader)}')\n",
    "print(f'Test Accuracy: {evaluate_model(test_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, but the loss function does decrease more reliably."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
