{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import gensim\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU:\n",
      "Quadro T1000 with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using GPU:\")\n",
    "    print(torch.cuda.get_device_name(0))  # 0 is the index of the GPU\n",
    "else:\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news = pd.read_pickle('..\\input\\df_news.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Algorithm: simple feed-forward neural network for binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Can be extended to include hidden layers\n",
    "- output of a neural unit: the sigmoid function which squished the outliers towards 0 or 1. \n",
    "Applicability: This architecture works well for problems where the relationship between the features and the output is approximately linear. For more complex relationships, you might use additional hidden layers or more sophisticated architectures.\n",
    "\n",
    "How it works:\n",
    "The input feature vector is passed through the linear layer (a dot product between input and weights plus a bias), followed by the sigmoid activation function which squashes the output to a value between 0 and 1.\n",
    "\n",
    "Loss Calculation: The BCELoss (Binary Cross Entropy Loss) measures the difference between the predicted probability and the actual binary label (0 or 1). It is used to train the model by minimizing this loss.\n",
    "\n",
    "Backpropagation: The optimizer (Adam in this case) adjusts the weights of the model to minimize the loss function through backpropagation.\n",
    "\n",
    "The 3Blue1Brown video is really good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader # this is for data loading\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class VectorizedTextDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        feature = torch.tensor(self.features[idx], dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return feature, label\n",
    "    \n",
    "df_training = df_news[['vector', 'subject', 'Real']]\n",
    "\n",
    "# Convert features to numpy arrays\n",
    "X = np.array(df_training['vector'].tolist()) # features - array of the vectors \n",
    "y = np.array(df_training['Real'].tolist()) # label - array of the 1 or 0s for each vector\n",
    "\n",
    "# Split data into test and train - here we do an 80/20 split. Optional: additional validation set \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create Dataset instances\n",
    "train_dataset = VectorizedTextDataset(X_train, y_train) # get the data in a structured dataset (abstract class that wraps around). \n",
    "test_dataset = VectorizedTextDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoader instances\n",
    "# this is for loading the data onto the GPU\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "class TextClassifier(nn.Module): # create a class with the neural networ module\n",
    "    def __init__(self, input_dim): # input dimensions\n",
    "        super(TextClassifier, ).__init__()\n",
    "        self.fc = nn.Linear(input_dim, 1) # nn.Linear (input, output) - dimensions of input (features) and output (labels)\n",
    "        self.sigmoid = nn.Sigmoid() # maps the output onto a 0 to 1 line, representing the probability of it belonging to a 1 \n",
    "    \n",
    "    def forward(self, x):  # forward nn \n",
    "        x = self.fc(x) # first the input is passed through the fully connected linear layer \n",
    "        x = self.sigmoid(x) # then it goes through the sigmoid \n",
    "        return x\n",
    "    \n",
    "# Model, Loss, Optimizer\n",
    "input_dim = X_train.shape[1]  # Number of features - 100 since this is the # components of the vector\n",
    "model = TextClassifier(input_dim)\n",
    "criterion = nn.BCELoss()  # loss function minimise \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # optimizer - lr learning rate, a critical hyperparameter that controls how large or small the updates to the weights will be at each step\n",
    "# Adam optimizer\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):  # choose epochs - one complete pass through the entire dataset\n",
    "    model.train()\n",
    "    for features, labels in train_loader:\n",
    "        optimizer.zero_grad() #  reset the gradients for each new epoch \n",
    "        outputs = model(features) \n",
    "        loss = criterion(outputs.squeeze(), labels.float())  # compute the loss\n",
    "        loss.backward()\n",
    "        optimizer.step() # update the weights \n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "def evaluate_model(loader):\n",
    "    model.eval() # model is in evaluate mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for features, labels in loader: # the loader is the dataset\n",
    "            outputs = model(features) # get the outputs\n",
    "            predicted = (outputs.squeeze() > 0.5).long()  # predicted label - it is assigned to whichever has a probability >0.5 (threshold can be changed)\n",
    "            correct += (predicted == labels).sum().item() \n",
    "            total += labels.size(0)\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "print(f'Train Accuracy: {evaluate_model(train_loader)}')\n",
    "print(f'Test Accuracy: {evaluate_model(test_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9564563728492678\n",
      "Test Accuracy: 0.9555679287305122\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model_2(loader):\n",
    "    model.eval() # model is in evaluate mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for features, labels in loader: # the loader is the dataset\n",
    "            outputs = model(features) # get the outputs\n",
    "            predicted = (outputs.squeeze() > 0.5).long()  # we only want to predict more than 0.8 - funny how you lose accuracy\n",
    "            correct += (predicted == labels).sum().item() \n",
    "            total += labels.size(0)\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "print(f'Train Accuracy: {evaluate_model_2(train_loader)}')\n",
    "print(f'Test Accuracy: {evaluate_model_2(test_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we use a hidden layer instead, does that improve the accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.10727664083242416\n",
      "Epoch 2, Loss: 0.00014839951472822577\n",
      "Epoch 3, Loss: 0.009782358072698116\n",
      "Epoch 4, Loss: 0.00034248080919496715\n",
      "Epoch 5, Loss: 0.00020904683333355933\n",
      "Train Accuracy: 0.9564563728492678\n",
      "Test Accuracy: 0.9555679287305122\n"
     ]
    }
   ],
   "source": [
    "class TextClassifier_hiddenlayer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(TextClassifier_hiddenlayer, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)  # First layer (input to hidden)\n",
    "        self.relu = nn.ReLU()  # Activation function for the hidden layer\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)  # Second layer (hidden to output)\n",
    "        self.sigmoid = nn.Sigmoid()  # Sigmoid for binary classification\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)  # Pass input through the first linear layer (input to hidden)\n",
    "        x = self.relu(x)  # Apply ReLU activation\n",
    "        x = self.fc2(x)  # Pass through the second linear layer (hidden to output)\n",
    "        x = self.sigmoid(x)  # Apply sigmoid activation for output\n",
    "        return x\n",
    "    \n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 100  # Example value, you can adjust it\n",
    "model_2 = TextClassifier_hiddenlayer(input_dim, hidden_dim)\n",
    "\n",
    "criterion = nn.BCELoss()  # loss function minimise \n",
    "optimizer = optim.Adam(model_2.parameters(), lr=0.001)  # optimizer - lr learning rate, a critical hyperparameter that controls how large or small the updates to the weights will be at each step\n",
    "# Adam optimizer\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):  # choose epochs - one complete pass through the entire dataset\n",
    "    model_2.train()\n",
    "    for features, labels in train_loader:\n",
    "        optimizer.zero_grad() #  reset the gradients for each new epoch \n",
    "        outputs = model_2(features) \n",
    "        loss = criterion(outputs.squeeze(), labels.float())  # compute the loss\n",
    "        loss.backward()\n",
    "        optimizer.step() # update the weights \n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "print(f'Train Accuracy: {evaluate_model_2(train_loader)}')\n",
    "print(f'Test Accuracy: {evaluate_model_2(test_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with tree classifier - Extreme Gradient Boosting\n",
    "\n",
    "Pros: faster to implement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "def run_xgboost(X_train, y_train, X_test, y_test):\n",
    "        \n",
    "        # initialize XGBoost classifier with class weights\n",
    "        model_xgb = xgb.XGBClassifier(scale_pos_weight=2, random_state=42)\n",
    "        model_xgb.fit(X_train,y_train)\n",
    "\n",
    "        # make predictions on the test set\n",
    "        y_pred = model_xgb.predict(X_test)\n",
    "\n",
    "        # print model evaluations\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        return "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
