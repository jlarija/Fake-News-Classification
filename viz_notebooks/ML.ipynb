{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A notebook for parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import gensim\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU:\n",
      "Quadro T1000 with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using GPU:\")\n",
    "    print(torch.cuda.get_device_name(0))  # 0 is the index of the GPU\n",
    "else:\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Inputs (not sure why shape mismatch TODO check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_news: Real\n",
      "0    0.522985\n",
      "1    0.477015\n",
      "Name: proportion, dtype: float64\n",
      "df_training: Real\n",
      "0    0.524278\n",
      "1    0.475722\n",
      "Name: proportion, dtype: float64\n",
      "df_test: Real\n",
      "0    0.517817\n",
      "1    0.482183\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_news = pd.read_pickle('..\\input\\df_news.pkl')\n",
    "\n",
    "# Use train_test_split for a more consistent and reproducible split\n",
    "df_training, df_test = train_test_split(df_news, test_size=0.2, random_state=42)\n",
    "\n",
    "# check that the df is balanced\n",
    "print('df_news:', df_news['Real'].value_counts(normalize=True))\n",
    "print('df_training:', df_training['Real'].value_counts(normalize=True))\n",
    "print('df_test:', df_test['Real'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader # this is for data loading\n",
    "\n",
    "df_training = df_training[['vector', 'subject', 'Real']]\n",
    "df_test = df_test[['vector', 'subject', 'Real']]\n",
    "\n",
    "# apply encoding consistently\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# fit LabelEncoder on the training set and transform both sets\n",
    "df_training['subject'] = label_encoder.fit_transform(df_training['subject'])\n",
    "df_test['subject'] = label_encoder.transform(df_test['subject'])  # Use transform, not fit_transform\n",
    "\n",
    "# we make a set of features for vector+subject and then a set of feature for vector only\n",
    "class VectorizedTextDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        feature = torch.tensor(self.features[idx], dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return feature, label\n",
    "    \n",
    "def create_data(df, mode='subj', batch_size=2, shuffle=False):\n",
    "    # Get the vectors (Word2Vec or equivalent)\n",
    "    X_vectors = np.array(df['vector'].tolist())\n",
    "\n",
    "    # If mode is 'subj', include subject features\n",
    "    if mode == 'subj':\n",
    "        df_features = df.drop(columns=['vector', 'Real'])\n",
    "        X_features = df_features.values\n",
    "        X = np.hstack([X_vectors, X_features])  # Concatenate vectors with subject features\n",
    "    else:\n",
    "        X = X_vectors  # Only use vectors\n",
    "    \n",
    "    y = np.array(df['Real'].tolist())  # Labels (1 or 0)\n",
    "    \n",
    "    # Create Dataset and DataLoader\n",
    "    input_dataset = VectorizedTextDataset(X, y)\n",
    "    input_loader = DataLoader(input_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    \n",
    "    return input_loader, X, y\n",
    "\n",
    "# Create DataLoaders for training and test sets\n",
    "train_loader_subj, X_train_subj, y_train_subj = create_data(df_training, mode='subj')\n",
    "test_loader_subj, X_test_subj, y_test_subj = create_data(df_test, mode='subj')\n",
    "\n",
    "train_loader, X_train, Y_train = create_data(df_training, mode='')\n",
    "test_loader, x_test, y_test = create_data(df_test, mode='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple feed-forward neural network for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module): # create a class with the neural networ module\n",
    "    def __init__(self, input_dim): # input dimensions\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, 1) # nn.Linear (input, output) - dimensions of input (features) and output (labels)\n",
    "        self.sigmoid = nn.Sigmoid() # maps the output onto a 0 to 1 line, representing the probability of it belonging to a 1 \n",
    "    \n",
    "    def forward(self, x):  # forward nn \n",
    "        x = self.fc(x) # first the input is passed through the fully connected linear layer \n",
    "        x = self.sigmoid(x) # then it goes through the sigmoid \n",
    "        return x\n",
    "    \n",
    "def model_training(input_loader, X_train, learning_rate, epochs):\n",
    "    # Model, Loss, Optimizer\n",
    "    input_dim = X_train.shape[1]  # Number of features - 100 since this is the # components of the vector\n",
    "    model = TextClassifier(input_dim)\n",
    "    criterion = nn.BCELoss()  # loss function minimise \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # optimizer - lr learning rate, a critical hyperparameter that controls how large or small the updates to the weights will be at each step\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):  # choose epochs - one complete pass through the entire dataset\n",
    "        model.train()\n",
    "        for features, labels in input_loader:  # input_loader is train_loader\n",
    "            optimizer.zero_grad() #  reset the gradients for each new epoch \n",
    "            outputs = model(features) \n",
    "            loss = criterion(outputs.squeeze(), labels.float())  # compute the loss\n",
    "            loss.backward()\n",
    "            optimizer.step() # update the weights \n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, loader, threshold):\n",
    "    model.eval() # model is in evaluate mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for features, labels in loader: # the loader is the dataset\n",
    "            outputs = model(features) # get the outputs\n",
    "            predicted = (outputs.squeeze() > threshold).long()  # predicted label - it is assigned to whichever has a probability >0.5 (threshold can be changed)\n",
    "            correct += (predicted == labels).sum().item() \n",
    "            total += labels.size(0)\n",
    "    accuracy = correct / total\n",
    "    print(f'Accuracy {accuracy}')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.00018206570530310273\n",
      "Epoch 2, Loss: 4.6553395804949105e-05\n",
      "Epoch 3, Loss: 5.472005796036683e-05\n",
      "Epoch 4, Loss: 6.729816959705204e-05\n",
      "Epoch 5, Loss: 6.0085090808570385e-05\n",
      "Epoch 1, Loss: 3.713506885105744e-05\n",
      "Epoch 2, Loss: 5.006815172237111e-06\n",
      "Epoch 3, Loss: 4.768373855768004e-07\n",
      "Epoch 4, Loss: 4.768373855768004e-07\n",
      "Epoch 5, Loss: 2.0861668872385053e-06\n"
     ]
    }
   ],
   "source": [
    "# train the model with the subject\n",
    "model = model_training(train_loader, X_train, learning_rate=0.1, epochs=5)\n",
    "model_subj = model_training(train_loader_subj, X_train_subj, learning_rate=0.1, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.9333760231638732\n",
      "Accuracy 0.9400579096831672\n"
     ]
    }
   ],
   "source": [
    "# evaluate model performances \n",
    "model_eval_subj = evaluate_model(model_subj, train_loader_subj, threshold=0.5)\n",
    "model_eval = evaluate_model(model, train_loader, threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.05407719314098358\n",
      "Epoch 2, Loss: 0.031646665185689926\n",
      "Epoch 3, Loss: 0.0234074704349041\n",
      "Epoch 4, Loss: 0.019186798483133316\n",
      "Epoch 5, Loss: 0.016677243635058403\n",
      "Epoch 1, Loss: 0.05170110613107681\n",
      "Epoch 2, Loss: 0.029679466038942337\n",
      "Epoch 3, Loss: 0.021408934146165848\n",
      "Epoch 4, Loss: 0.017055576667189598\n",
      "Epoch 5, Loss: 0.014419246464967728\n",
      "Accuracy 0.9556489782281864\n",
      "Accuracy 0.9542012361490061\n"
     ]
    }
   ],
   "source": [
    "def new_lr(learningrate):\n",
    "    # train the model with the subject\n",
    "    model = model_training(train_loader, X_train, learning_rate=learningrate, epochs=5)\n",
    "    model_subj = model_training(train_loader_subj, X_train_subj, learning_rate=learningrate, epochs=5)\n",
    "    model_eval_subj = evaluate_model(model_subj, train_loader_subj, threshold=0.5)\n",
    "    model_eval = evaluate_model(model, train_loader, threshold=0.5)\n",
    "    return\n",
    "new_lr(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing how the code converges, changing the number of epochs would not make it improve. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would adding a hidden layer improve the accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.10727664083242416\n",
      "Epoch 2, Loss: 0.00014839951472822577\n",
      "Epoch 3, Loss: 0.009782358072698116\n",
      "Epoch 4, Loss: 0.00034248080919496715\n",
      "Epoch 5, Loss: 0.00020904683333355933\n",
      "Train Accuracy: 0.9564563728492678\n",
      "Test Accuracy: 0.9555679287305122\n"
     ]
    }
   ],
   "source": [
    "class TextClassifier_hiddenlayer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(TextClassifier_hiddenlayer, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)  # First layer (input to hidden)\n",
    "        self.relu = nn.ReLU()  # Activation function for the hidden layer\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)  # Second layer (hidden to output)\n",
    "        self.sigmoid = nn.Sigmoid()  # Sigmoid for binary classification\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)  # Pass input through the first linear layer (input to hidden)\n",
    "        x = self.relu(x)  # Apply ReLU activation\n",
    "        x = self.fc2(x)  # Pass through the second linear layer (hidden to output)\n",
    "        x = self.sigmoid(x)  # Apply sigmoid activation for output\n",
    "        return x\n",
    "    \n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 100  # Example value, you can adjust it\n",
    "model_2 = TextClassifier_hiddenlayer(input_dim, hidden_dim)\n",
    "\n",
    "criterion = nn.BCELoss()  # loss function minimise \n",
    "optimizer = optim.Adam(model_2.parameters(), lr=0.001)  # optimizer - lr learning rate, a critical hyperparameter that controls how large or small the updates to the weights will be at each step\n",
    "# Adam optimizer\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):  # choose epochs - one complete pass through the entire dataset\n",
    "    model_2.train()\n",
    "    for features, labels in train_loader:\n",
    "        optimizer.zero_grad() #  reset the gradients for each new epoch \n",
    "        outputs = model_2(features) \n",
    "        loss = criterion(outputs.squeeze(), labels.float())  # compute the loss\n",
    "        loss.backward()\n",
    "        optimizer.step() # update the weights \n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "print(f'Train Accuracy: {evaluate_model(train_loader)}')\n",
    "print(f'Test Accuracy: {evaluate_model(test_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No. The data is probably too simple to benefit from a hidden layer (hidden layers are more useful in image recognition tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does XGBoost perform instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters that can be tuned: \n",
    "- estimators\n",
    "- learning rate\n",
    "- tree depth\n",
    "- scale_pos_weight (imbalanced classes)\n",
    "\n",
    "Alternatives: \n",
    "use scikit optimizer like grisearchCV to tune these parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9793\n",
      "F1 Score: 0.9785\n",
      "Precision: 0.9781\n",
      "Recall: 0.9790\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score \n",
    "\n",
    "model_xgb = xgb.XGBClassifier(scale_pos_weight=2, random_state=42, n_estimators=500)\n",
    "model_xgb.fit(X_train, Y_train)\n",
    "\n",
    "# make predictions on the test set\n",
    "y_pred = model_xgb.predict(x_test)\n",
    "y_actual = y_test\n",
    "# xgboost comparison\n",
    "accuracy = accuracy_score(y_actual, y_pred)\n",
    "f1 = f1_score(y_actual, y_pred)\n",
    "precision = precision_score(y_actual, y_pred)\n",
    "recall = recall_score(y_actual, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
